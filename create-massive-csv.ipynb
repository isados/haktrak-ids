{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import dtype\n",
    "import numpy as np\n",
    "\n",
    "dtypes = {'Timestamp': dtype('O'),\n",
    " 'Dst Port': dtype('int64'),\n",
    " 'Protocol': dtype('int64'),\n",
    " 'Flow Duration': dtype('int64'),\n",
    " 'Tot Fwd Pkts': dtype('int64'),\n",
    " 'Tot Bwd Pkts': dtype('int64'),\n",
    " 'TotLen Fwd Pkts': dtype('float64'),\n",
    " 'TotLen Bwd Pkts': dtype('float64'),\n",
    " 'Fwd Pkt Len Max': dtype('float64'),\n",
    " 'Fwd Pkt Len Min': dtype('float64'),\n",
    " 'Fwd Pkt Len Mean': dtype('float64'),\n",
    " 'Fwd Pkt Len Std': dtype('float64'),\n",
    " 'Bwd Pkt Len Max': dtype('float64'),\n",
    " 'Bwd Pkt Len Min': dtype('float64'),\n",
    " 'Bwd Pkt Len Mean': dtype('float64'),\n",
    " 'Bwd Pkt Len Std': dtype('float64'),\n",
    " 'Flow Byts/s': dtype('float64'),\n",
    " 'Flow Pkts/s': dtype('float64'),\n",
    " 'Flow IAT Mean': dtype('float64'),\n",
    " 'Flow IAT Std': dtype('float64'),\n",
    " 'Flow IAT Max': dtype('float64'),\n",
    " 'Flow IAT Min': dtype('float64'),\n",
    " 'Fwd IAT Tot': dtype('float64'),\n",
    " 'Fwd IAT Mean': dtype('float64'),\n",
    " 'Fwd IAT Std': dtype('float64'),\n",
    " 'Fwd IAT Max': dtype('float64'),\n",
    " 'Fwd IAT Min': dtype('float64'),\n",
    " 'Bwd IAT Tot': dtype('float64'),\n",
    " 'Bwd IAT Mean': dtype('float64'),\n",
    " 'Bwd IAT Std': dtype('float64'),\n",
    " 'Bwd IAT Max': dtype('float64'),\n",
    " 'Bwd IAT Min': dtype('float64'),\n",
    " 'Fwd PSH Flags': dtype('int64'),\n",
    " 'Bwd PSH Flags': dtype('int64'),\n",
    " 'Fwd URG Flags': dtype('int64'),\n",
    " 'Bwd URG Flags': dtype('int64'),\n",
    " 'Fwd Header Len': dtype('int64'),\n",
    " 'Bwd Header Len': dtype('int64'),\n",
    " 'Fwd Pkts/s': dtype('float64'),\n",
    " 'Bwd Pkts/s': dtype('float64'),\n",
    " 'Pkt Len Min': dtype('float64'),\n",
    " 'Pkt Len Max': dtype('float64'),\n",
    " 'Pkt Len Mean': dtype('float64'),\n",
    " 'Pkt Len Std': dtype('float64'),\n",
    " 'Pkt Len Var': dtype('float64'),\n",
    " 'FIN Flag Cnt': dtype('int64'),\n",
    " 'SYN Flag Cnt': dtype('int64'),\n",
    " 'RST Flag Cnt': dtype('int64'),\n",
    " 'PSH Flag Cnt': dtype('int64'),\n",
    " 'ACK Flag Cnt': dtype('int64'),\n",
    " 'URG Flag Cnt': dtype('int64'),\n",
    " 'CWE Flag Count': dtype('int64'),\n",
    " 'ECE Flag Cnt': dtype('int64'),\n",
    " 'Down/Up Ratio': dtype('float64'),\n",
    " 'Pkt Size Avg': dtype('float64'),\n",
    " 'Fwd Seg Size Avg': dtype('float64'),\n",
    " 'Bwd Seg Size Avg': dtype('float64'),\n",
    " 'Fwd Byts/b Avg': dtype('float64'),\n",
    " 'Fwd Pkts/b Avg': dtype('float64'),\n",
    " 'Fwd Blk Rate Avg': dtype('float64'),\n",
    " 'Bwd Byts/b Avg': dtype('float64'),\n",
    " 'Bwd Pkts/b Avg': dtype('float64'),\n",
    " 'Bwd Blk Rate Avg': dtype('float64'),\n",
    " 'Subflow Fwd Pkts': dtype('int64'),\n",
    " 'Subflow Fwd Byts': dtype('int64'),\n",
    " 'Subflow Bwd Pkts': dtype('int64'),\n",
    " 'Subflow Bwd Byts': dtype('int64'),\n",
    " 'Init Fwd Win Byts': dtype('int64'),\n",
    " 'Init Bwd Win Byts': dtype('int64'),\n",
    " 'Fwd Act Data Pkts': dtype('int64'),\n",
    " 'Fwd Seg Size Min': dtype('int64'),\n",
    " 'Active Mean': dtype('float64'),\n",
    " 'Active Std': dtype('float64'),\n",
    " 'Active Max': dtype('float64'),\n",
    " 'Active Min': dtype('float64'),\n",
    " 'Idle Mean': dtype('float64'),\n",
    " 'Idle Std': dtype('float64'),\n",
    " 'Idle Max': dtype('float64'),\n",
    " 'Idle Min': dtype('float64'),\n",
    " 'Label': dtype('O')}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-de3ca56981bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m from pandas.core.api import (\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;31m# dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mInt8Dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedAgg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m from pandas.core.indexes.api import (\n\u001b[1;32m     33\u001b[0m     \u001b[0mCategoricalIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/groupby/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedAgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroupBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = [\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas.core.groupby.groupby import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mGroupBy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0m_agg_template\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlibgroupby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m from pandas._typing import (\n\u001b[1;32m     42\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/groupby.pyx\u001b[0m in \u001b[0;36minit pandas._libs.groupby\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Creating the CSV\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "list_of_files = glob(\"/home/haktrak/Public/full_datasets/shuffled.csv\")\n",
    "classes_andcounts = {}\n",
    "LIMIT_PER_CLASS = 100\n",
    "\n",
    "header = True\n",
    "mode = 'w'\n",
    "cols = list(dtypes.keys())\n",
    "\n",
    "for csv_file in list_of_files:\n",
    "    print(f\"#### {os.path.basename(csv_file)} ####\")\n",
    "    chunks = pd.read_csv(csv_file,chunksize=100000, usecols=cols, low_memory=False)\n",
    "\n",
    "    for num, chunk in enumerate(chunks):\n",
    "        start = time.time()\n",
    "        # Cleaning\n",
    "\n",
    "        chunk.dropna(axis=0, inplace=True)\n",
    "\n",
    "        repeated_headers = chunk[(chunk.Protocol == 'Protocol')].index\n",
    "        chunk.drop(repeated_headers, axis=0, inplace=True)\n",
    "        # Convert to the right type\n",
    "        # chunk.astype(dtypes)\n",
    "\n",
    "        # Getting the classes\n",
    "        chunk_counts = chunk['Label'].value_counts().to_dict()\n",
    "        for key, count in chunk_counts.items():\n",
    "            classes_andcounts[key] = classes_andcounts.get(key, 0) + count\n",
    "        \n",
    "        # Writing it to csv\n",
    "        chunk[cols].to_csv(\"csv/full_dataset.csv\", header=header, columns=cols, mode=mode, index=False)\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"Processed chuck #{num}: \",(end-start),\"sec\")\n",
    "        header=False\n",
    "        mode = 'a'\n",
    "    print(classes_andcounts)    \n",
    "    print()\n",
    "# print(classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"{'DDoS attacks-LOIC-HTTP': 576191, 'Benign': 13425831, 'Brute Force -Web': 611, 'Brute Force -XSS': 230, 'SQL Injection': 87, 'DDOS attack-HOIC': 686012, 'DDOS attack-LOIC-UDP': 1730, 'DoS attacks-SlowHTTPTest': 139890, 'DoS attacks-Hulk': 461912, 'Bot': 286191, 'FTP-BruteForce': 193354, 'SSH-Bruteforce': 187589, 'Infilteration': 161096, 'DoS attacks-GoldenEye': 41508, 'DoS attacks-Slowloris': 10990}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#### shuffled.csv ####\n",
      "Processed chunk #0...\n",
      "Processed chunk #1...\n",
      "Processed chunk #2...\n",
      "Processed chunk #3...\n",
      "Processed chunk #4...\n",
      "Processed chunk #5...\n",
      "Processed chunk #6...\n",
      "Processed chunk #7...\n",
      "Processed chunk #8...\n",
      "Processed chunk #9...\n",
      "Processed chunk #10...\n",
      "Processed chunk #11...\n",
      "Processed chunk #12...\n",
      "Processed chunk #13...\n",
      "Processed chunk #14...\n",
      "Processed chunk #15...\n",
      "Processed chunk #16...\n",
      "Processed chunk #17...\n",
      "Processed chunk #18...\n",
      "Processed chunk #19...\n",
      "Processed chunk #20...\n",
      "Processed chunk #21...\n",
      "Processed chunk #22...\n",
      "Processed chunk #23...\n",
      "Processed chunk #24...\n",
      "Processed chunk #25...\n",
      "Processed chunk #26...\n",
      "Processed chunk #27...\n",
      "Processed chunk #28...\n",
      "Processed chunk #29...\n",
      "Processed chunk #30...\n",
      "Processed chunk #31...\n",
      "Processed chunk #32...\n",
      "Processed chunk #33...\n",
      "Processed chunk #34...\n",
      "Processed chunk #35...\n",
      "Processed chunk #36...\n",
      "Processed chunk #37...\n",
      "Processed chunk #38...\n",
      "Processed chunk #39...\n",
      "Processed chunk #40...\n",
      "Processed chunk #41...\n",
      "Processed chunk #42...\n",
      "Processed chunk #43...\n",
      "Processed chunk #44...\n",
      "Processed chunk #45...\n",
      "Processed chunk #46...\n",
      "Processed chunk #47...\n",
      "Processed chunk #48...\n",
      "Processed chunk #49...\n",
      "Processed chunk #50...\n",
      "Processed chunk #51...\n",
      "Processed chunk #52...\n",
      "Processed chunk #53...\n",
      "Processed chunk #54...\n",
      "Processed chunk #55...\n",
      "Processed chunk #56...\n",
      "Processed chunk #57...\n",
      "Processed chunk #58...\n",
      "Processed chunk #59...\n",
      "Processed chunk #60...\n",
      "Processed chunk #61...\n",
      "Processed chunk #62...\n",
      "Processed chunk #63...\n",
      "Processed chunk #64...\n",
      "Processed chunk #65...\n",
      "Processed chunk #66...\n",
      "Processed chunk #67...\n",
      "Processed chunk #68...\n",
      "Processed chunk #69...\n",
      "Processed chunk #70...\n",
      "Processed chunk #71...\n",
      "Processed chunk #72...\n",
      "Processed chunk #73...\n",
      "Processed chunk #74...\n",
      "Processed chunk #75...\n",
      "Processed chunk #76...\n",
      "Processed chunk #77...\n",
      "Processed chunk #78...\n",
      "Processed chunk #79...\n",
      "Processed chunk #80...\n",
      "Processed chunk #81...\n",
      "Processed chunk #82...\n",
      "Processed chunk #83...\n",
      "Processed chunk #84...\n",
      "Processed chunk #85...\n",
      "Processed chunk #86...\n",
      "Processed chunk #87...\n",
      "Processed chunk #88...\n",
      "Processed chunk #89...\n",
      "Processed chunk #90...\n",
      "Processed chunk #91...\n",
      "Processed chunk #92...\n",
      "Processed chunk #93...\n",
      "Processed chunk #94...\n",
      "Processed chunk #95...\n"
     ]
    }
   ],
   "source": [
    "### Removing certain classes and create a balanced dataset\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "list_of_files = glob(\"/home/haktrak/Public/full_datasets/shuffled.csv\")\n",
    "\n",
    "LIMIT_PER_CLASS = 1000\n",
    "TRAIN_TEST_SPLIT = 0.7\n",
    "split_num = int(LIMIT_PER_CLASS*TRAIN_TEST_SPLIT)\n",
    "\n",
    "classes = {'DDoS attacks-LOIC-HTTP': 0,\n",
    " 'Benign': 0,\n",
    " 'Brute Force -Web': 0,\n",
    " 'Brute Force -XSS': 0,\n",
    " 'SQL Injection': 0,\n",
    " 'DDOS attack-HOIC': 0,\n",
    " 'DDOS attack-LOIC-UDP': 0,\n",
    " 'DoS attacks-SlowHTTPTest': 0,\n",
    " 'DoS attacks-Hulk': 0,\n",
    " 'Bot': 0,\n",
    " 'FTP-BruteForce': 0,\n",
    " 'SSH-Bruteforce': 0,\n",
    " 'Infilteration': 0,\n",
    " 'DoS attacks-GoldenEye': 0,\n",
    " 'DoS attacks-Slowloris': 0}\n",
    "\n",
    "# classes = list(classes.keys())\n",
    "[classes.pop(i) for i in ['Brute Force -Web', 'Brute Force -XSS', 'SQL Injection']]\n",
    "\n",
    "\n",
    "header = True\n",
    "mode = 'w'\n",
    "cols = list(dtypes.keys())\n",
    "\n",
    "for csv_file in list_of_files:\n",
    "    print(f\"#### {os.path.basename(csv_file)} ####\")\n",
    "    chunks = pd.read_csv(csv_file,chunksize=100000, usecols=cols, dtype=dtypes, low_memory=False)\n",
    "    empty_df = pd.DataFrame(columns=cols)\n",
    "    \n",
    "    for num, chunk in enumerate(chunks):\n",
    "        start = time.time()\n",
    "        \n",
    "        train_chunk = empty_df.copy()\n",
    "        test_chunk = empty_df.copy()\n",
    "        \n",
    "        # Getting the classes\n",
    "        chunk_counts = chunk['Label'].value_counts().to_dict()\n",
    "        for label, count_in_chunk in chunk_counts.items():\n",
    "            previous_count = classes.get(label)\n",
    "\n",
    "            if previous_count == LIMIT_PER_CLASS:\n",
    "                classes.pop(label)\n",
    "                continue\n",
    "            if previous_count == None:\n",
    "                continue\n",
    "\n",
    "            rows_to_grab = LIMIT_PER_CLASS - previous_count\n",
    "            if rows_to_grab > count_in_chunk:\n",
    "                rows_to_grab = count_in_chunk\n",
    "            \n",
    "            \n",
    "            classes[label] += rows_to_grab\n",
    "            new_chunk = new_chunk.append(chunk[chunk['Label']==label][:rows_to_grab], ignore_index=True)\n",
    "\n",
    "        # Writing it to csv\n",
    "        new_chunk.to_csv(f\"/home/haktrak/Public/full_datasets/shuffled_{LIMIT_PER_CLASS}.csv\", header=header, columns=cols, mode=mode, index=False)\n",
    "        end = time.time()\n",
    "        header=False\n",
    "        mode = 'a'\n",
    "        print(f\"Processed chunk #{num}...\")\n",
    "        if len(classes.keys()) == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DDOS attack-HOIC            1000\n",
       "Bot                         1000\n",
       "DoS attacks-SlowHTTPTest    1000\n",
       "DDoS attacks-LOIC-HTTP      1000\n",
       "DDOS attack-LOIC-UDP        1000\n",
       "DoS attacks-GoldenEye       1000\n",
       "Infilteration               1000\n",
       "DoS attacks-Hulk            1000\n",
       "SSH-Bruteforce              1000\n",
       "FTP-BruteForce              1000\n",
       "DoS attacks-Slowloris       1000\n",
       "Benign                      1000\n",
       "Name: Label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "path = \"/home/haktrak/Public/full_datasets/shuffled_1000.csv\"\n",
    "df = pd.read_csv(path, dtype=dtypes)\n",
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "x = df.drop('Label', axis=1)\n",
    "y = df.Label\n",
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=0.7, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 2295  7203 10428 ... 11670  5438  2700] [3251 9580 9734 ... 5140 5935 6741]\n"
     ]
    }
   ],
   "source": [
    "gen = sss.split(x,y)\n",
    "for train, test in gen:\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[train]\n",
    "test_df = df.iloc[test]\n",
    "train_df.to_csv(f\"/home/haktrak/Public/full_datasets/shuffled_1000_train.csv\", header=True, index=False)\n",
    "test_df.to_csv(f\"/home/haktrak/Public/full_datasets/shuffled_1000_test.csv\", header=True, index=False)"
   ]
  }
 ]
}